{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0iQxLXRAJfh+cdfDJOBmg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elahe-ghafari/my-final-project/blob/main/VGG19%2BViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZujeUT2O2TEr"
      },
      "outputs": [],
      "source": [
        "import os                       # for working with files\n",
        "\n",
        "import numpy as np              # for numerical computationss\n",
        "import pandas as pd             # for working with dataframes\n",
        "import seaborn as sns\n",
        "import torch                    # Pytorch module\n",
        "import matplotlib.pyplot as plt # for plotting informations on graph and images using tensors\n",
        "import torch.nn as nn           # for creating  neural networks\n",
        "from torch.utils.data import DataLoader # for dataloaders\n",
        "from PIL import Image           # for checking images\n",
        "import torch.nn.functional as F # for functions for calculating loss\n",
        "import torchvision.transforms as transforms   # for transforming images into tensors\n",
        "from torchvision.utils import make_grid       # for data checking\n",
        "from torchvision.datasets import ImageFolder  # for working with classes and images\n",
        "from torchsummary import summary              # for getting the summary of our model\n",
        "import tensorflow as ts\n",
        "from  tensorflow import keras\n",
        "import itertools\n",
        "from sklearn.metrics import precision_score, accuracy_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "U_IrG7WG3EMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile ('/content/drive/My Drive/melanom.zip','r') as zipObj:\n",
        "  zipObj.extractall('melanom')"
      ],
      "metadata": {
        "id": "26TLod4Z3Ijp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "!zip -r /content/melanom.zip /content/melanom\n",
        "files.download('/content/drive/My Drive/melanom.zip')"
      ],
      "metadata": {
        "id": "CfOGh8453MwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'melanom/melanoma_cancer_dataset/train'\n",
        "skin = os.listdir(train_dir)\n",
        "skin"
      ],
      "metadata": {
        "id": "6U38JL113VG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of images for each diseas\n",
        "nums_train = {}\n",
        "nums_val = {}\n",
        "for s in skin:\n",
        "    nums_train[s] = len(os.listdir(train_dir + '/' + s))\n",
        "img_per_class_train = pd.DataFrame(nums_train.values(), index=nums_train.keys(), columns=[\"no. of images\"])\n",
        "print('Train data distribution :')\n",
        "img_per_class_train"
      ],
      "metadata": {
        "id": "Hp-Mkf7A3buU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.title('data distribution ',fontsize=30)\n",
        "plt.ylabel('Number of image',fontsize=20)\n",
        "plt.xlabel('Type of skin cancer',fontsize=20)\n",
        "\n",
        "keys = list(nums_train.keys())\n",
        "vals = list(nums_train.values())\n",
        "sns.barplot(x=keys, y=vals)"
      ],
      "metadata": {
        "id": "X_xYfR2v3dak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to show image\n",
        "train = ImageFolder(train_dir, transform=transforms.ToTensor())\n",
        "def show_image(image, label):\n",
        "    print(\"Label :\" + train.classes[label] + \"(\" + str(label) + \")\")\n",
        "    return image.permute(1, 2, 0)"
      ],
      "metadata": {
        "id": "cI6H5rTm3kEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modeling"
      ],
      "metadata": {
        "id": "T-P6mGhD3pZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
        "                                                         rotation_range = 0.30 ,\n",
        "                                                         horizontal_flip = True ,\n",
        "                                                         validation_split = 0.2\n",
        "                                                         )\n",
        "valid_gen =  keras.preprocessing.image.ImageDataGenerator(rescale=1./255,validation_split = 0.2)\n",
        "train_data = train_gen.flow_from_directory(train_dir, subset='training', target_size=(224,224), batch_size=64, color_mode='rgb',\n",
        "                                            class_mode='categorical', shuffle=True)\n",
        "\n",
        "test_data = valid_gen.flow_from_directory(train_dir, subset='validation', target_size=(224,224), batch_size=64, color_mode='rgb',\n",
        "                                            class_mode='categorical', shuffle=False)"
      ],
      "metadata": {
        "id": "XAlnGYAi3rRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL vgg19+VIT"
      ],
      "metadata": {
        "id": "VPCpCOTw3xjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Reshape\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.applications import VGG19\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dense, Dropout\n",
        "\n",
        "# Define the CNN backbone (you can use pre-trained models such as VGG16)\n",
        "def cnn_backbone(input_shape):\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = True\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    return Model(base_model.input, x)\n",
        "\n",
        "# Define the Vision Transformer component\n",
        "def vision_transformer(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    x = Reshape((-1, input_shape[2]))(input_layer)  # Flatten spatial dimensions\n",
        "    x = LayerNormalization()(x)\n",
        "    x = MultiHeadAttention(num_heads=8, key_dim=32, dropout=0.1)([x, x])\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dense(256, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    return Model(input_layer, x)\n",
        "\n",
        "# Input shape\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "# Define the CNN backbone\n",
        "cnn_model = cnn_backbone(input_shape)\n",
        "\n",
        "# Define the Vision Transformer component\n",
        "vit_model = vision_transformer(cnn_model.output_shape[1:])\n",
        "\n",
        "# Combine the CNN backbone and Vision Transformer component\n",
        "cnn_output = cnn_model.output\n",
        "vit_output = vit_model(cnn_output)\n",
        "\n",
        "# Define the final model\n",
        "final_output = Dense(1, activation=\"sigmoid\")(vit_output) # For binary classification (melanoma or not)\n",
        "hybrid_model = Model(inputs=cnn_model.input, outputs=final_output)\n",
        "\n",
        "# Compile the model\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "hybrid_model.compile(optimizer=opt, loss=binary_crossentropy, metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "hybrid_model.summary()\n"
      ],
      "metadata": {
        "id": "OIkYcjn032g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "keras.utils.plot_model(\n",
        "    model_vgg16,\n",
        "    to_file=\"model.png\",\n",
        "    show_shapes=False,\n",
        "    show_dtype=False,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        "    layer_range=None,\n",
        "    show_layer_activations=False,\n",
        ")"
      ],
      "metadata": {
        "id": "DcX_cj0O4QWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# Define Early Stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Define other parameters and compile the model\n",
        "# Assuming you have already defined and compiled your model\n",
        "\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model_vgg16.fit(train_data, epochs=10, validation_data=test_data, callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "KQ0N1RhK4Soj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Train and Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(history.history['loss'],label=\"Train Loss\")\n",
        "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
        "plt.xlim(0, 10)\n",
        "plt.ylim(0.0,1.0)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Train and Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.plot(history.history['accuracy'], label=\"Train Accuracy\")\n",
        "plt.plot(history.history['val_accuracy'], label=\"Validation Accuracy\")\n",
        "plt.xlim(0, 9.25)\n",
        "plt.ylim(0.75,1.0)\n",
        "plt.legend()\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "WV_jEYBK4YQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Z8c3zukV4cuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "Y_pred = model_vgg16.predict(test_data)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print(classification_report(test_data.classes, y_pred))"
      ],
      "metadata": {
        "id": "FdI2uRxJ4g2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "# calculating and plotting the confusion matrix\n",
        "cm1 = confusion_matrix(test_data.classes, y_pred)\n",
        "plot_confusion_matrix(conf_mat=cm1,show_absolute=True,\n",
        "                                show_normed=True,\n",
        "                                colorbar=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A16UD6n74niH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}